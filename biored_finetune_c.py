# -*- coding: utf-8 -*-
"""biored_finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ezTeh2f0b7Jr_70awRWJkTNhBmnsu6Be
"""

# biored_finetune.py
"""
Endâ€‘toâ€‘end Colab script for fineâ€‘tuning PubMedBERT on the BioRED binaryâ€‘relation task
using memoryâ€‘saving settings that work in a free Colab GPU session (â‰ˆ12â€¯GB RAM).

âœ“ mounts Drive
âœ“ installs nothing (assumes runtime already has transformersâ€¯â‰¥â€¯4.55, datasetsâ€¯â‰¥â€¯2.19)
âœ“ tokenises train/dev/test TSVs (train_binary.tsv etc.)
âœ“ removes unused columns & switches to lazy Torch format
âœ“ computes pos_weight for classâ€‘imbalance
âœ“ trains with small batchÂ 4 + gradâ€‘accumÂ 4 (effective 16)
âœ“ evaluates & runs a demo inference
"""

import os, numpy as np, torch, torch.nn as nn
from google.colab import drive
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer,
)
from sklearn.metrics import (
    precision_recall_fscore_support,
    accuracy_score,
    roc_auc_score,
)

# ---------------------------- 0. Mount Drive ----------------------------
drive.mount("/content/drive")

# ---------------------------- 1. Paths & constants ----------------------
DATA_DIR = "/content/drive/MyDrive/BioRED"  # <-- adjust if needed
MODEL_NAME = "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"
BATCH_SIZE = 32
GRAD_ACCUM = 1
EPOCHS = 2

# ---------------------------- 2. Load TSVs ------------------------------

def load_split(split: str):
    path = os.path.join(DATA_DIR, f"{split}_binary.tsv")
    return load_dataset("csv", data_files=path, delimiter="\t")['train']

train_ds = load_split("train")
dev_ds   = load_split("dev")
test_ds  = load_split("test")
print("Loaded", len(train_ds), "train /", len(dev_ds), "dev /", len(test_ds), "test")

# ---------------------------- 3. Tokenisation ---------------------------
tok = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize(batch):
    return tok(batch["sentence"], truncation=True, padding="max_length", max_length=512)

for name, ds in [("train", train_ds), ("dev", dev_ds), ("test", test_ds)]:
    if "sentence" in ds.column_names:      # test might already be tokenised in older runs
        ds = ds.map(tokenize, batched=True, num_proc=4 if name=="train" else 2)
    if "label" in ds.column_names:
        ds = ds.rename_column("label", "labels")
    # remove unused text / entity columns if still present
    cols_to_drop = [c for c in ['sentence','h','h_type','t','t_type','token_type_ids'] if c in ds.column_names]
    ds = ds.remove_columns(cols_to_drop)
    # keep as lazy Torch tensors â†’ huge RAM saving
    ds.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
    if name=="train": train_ds=ds
    elif name=="dev": dev_ds=ds
    else: test_ds=ds
print("Final columns:", train_ds.column_names)

# ---------------------------- 4. pos_weight -----------------------------
pos = torch.tensor(train_ds["labels"]).sum()
neg = len(train_ds) - pos
pos_weight = torch.tensor([neg/pos])
print("pos_weight =", pos_weight.item())

# ---------------------------- 5. Model ----------------------------------
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)

# ---------------------------- 6. Custom Trainer -------------------------
class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **_):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(outputs.logits.device))
        loss = loss_fn(outputs.logits, torch.nn.functional.one_hot(labels,2).float())
        return (loss, outputs) if return_outputs else loss

def compute_metrics(pred):
    logits, labels = pred
    preds = np.argmax(logits, axis=-1)
    p,r,f1,_ = precision_recall_fscore_support(labels, preds, average="binary")
    acc = accuracy_score(labels, preds)
    auc = roc_auc_score(labels, logits[:,1])
    return {"precision":p, "recall":r, "f1":f1, "accuracy":acc, "auc":auc}

# ---------------------------- 7. TrainingArguments ----------------------
args = TrainingArguments(
    output_dir="biored_pubmedbert_bin",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    gradient_accumulation_steps=1,
    num_train_epochs=EPOCHS,
    fp16=True,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to="none",   # no W&B
)

trainer = WeightedTrainer(
    model=model,
    args=args,
    train_dataset=train_ds,
    eval_dataset=dev_ds,
    compute_metrics=compute_metrics,
)

# ---------------------------- 8. Train & evaluate -----------------------
trainer.train()
print("Dev metrics:", trainer.evaluate())
print("Test metrics:", trainer.evaluate(test_ds))

# å­¦ç¿’ç›´å¾Œã«ä¿å­˜ï¼ˆæ¨å¥¨ï¼‰
import os, json
SAVE_DIR = "/content/drive/MyDrive/BioRED/models/pubmedbert_bin_best"
os.makedirs(SAVE_DIR, exist_ok=True)

# ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ Drive ã«ä¿å­˜
trainer.save_model(SAVE_DIR)     # â† model ã¨ config ã‚’ä¿å­˜
tok.save_pretrained(SAVE_DIR)    # â† tokenizer ã‚‚åŒã˜å ´æ‰€ã«ä¿å­˜

# é–¾å€¤ã‚‚ãƒ¡ãƒ¢ã—ã¦ãŠãï¼ˆä¾‹ï¼šDevã§æ±‚ã‚ãŸ best_thï¼‰
with open(os.path.join(SAVE_DIR, "threshold.json"), "w") as f:
    json.dump({"best_logit_th": float(best_th)}, f)

print("Saved to:", SAVE_DIR)

"""è£œè¶³ï¼šå­¦ç¿’å†é–‹ã‚’è¦‹è¶Šã™ãªã‚‰ trainer.save_state() ã‚‚ä½µã›ã¦ä¿å­˜ã—ã¦ãŠãã¨ã€
trainer.train(resume_from_checkpoint=True) ã§ ç¶šãã‹ã‚‰å†é–‹ã§ãã¾ã™ã€‚
"""

# ãƒ¢ãƒ‡ãƒ«ã¨å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä¸¡æ–¹ã¨ã‚‚åŒã˜ãƒ‡ãƒã‚¤ã‚¹ï¼ˆGPU ãªã‚‰ cudaï¼‰ã«è¼‰ã›ã‚‹ã‚ˆã†ã«ä¿®æ­£
import torch

# â‘  ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ç§»å‹•
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# â‘¡ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºâ†’GPU ã«è»¢é€
example = "IL-6 causes fever in inflammatory responses."
inputs = tok(
    example,
    return_tensors="pt",
    truncation=True,
    max_length=512
)
# all tensors to same device
inputs = {k: v.to(device) for k, v in inputs.items()}

# â‘¢ æ¨è«–ï¼†ç¢ºç‡è¨ˆç®—
with torch.no_grad():
    logits = model(**inputs).logits
    proba  = torch.softmax(logits, dim=1)[0, 1].item()

print(f"Relation probability: {proba:.3f}")

# Dev ã§ã®ã™ã¹ã¦ã®äºˆæ¸¬ã‚¹ã‚³ã‚¢ã‚’å–å¾—
preds = trainer.predict(dev_ds)
scores = preds.predictions[:,1]
labels = preds.label_ids
# sklearn ãªã©ã§ precision_recall_curve ã‚’æã
from sklearn.metrics import precision_recall_curve
prec, rec, ths = precision_recall_curve(labels, scores)

# å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer

DATA_DIR = "/content/drive/MyDrive/BioRED"
dev_ds = load_dataset("csv", data_files=f"{DATA_DIR}/dev_binary.tsv", delimiter="\t")["train"]
# æ—¢å­˜ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼†æ•´å½¢é–¢æ•°ã‚’ã“ã“ã«å†é©ç”¨

# config.json ã¨ pytorch_model.binï¼ˆã¾ãŸã¯ model.safetensorsï¼‰ã®ç¢ºèª
import os, glob
for p in [
    "/content/drive/MyDrive/BioRED/biored_pubmedbert_bin",
    "/content/biored_pubmedbert_bin"
]:
    print("DIR?", p, os.path.isdir(p))
    if os.path.isdir(p):
        print("FILES:", [os.path.basename(x) for x in glob.glob(os.path.join(p, "*"))][:10])

# å­¦ç¿’ã¯ã‚„ã‚Šç›´ã•ãšã€ã€ŒDev/Test ã‚’èª­ã¿è¾¼ã¿â†’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºâ†’è©•ä¾¡ã€ã ã‘ã‚„ã‚Šç›´ã—
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score

DATA_DIR = "/content/drive/MyDrive/BioRED"
CKPT = "/content/biored_pubmedbert_bin/checkpoint-15708"  # ç›´å‰ã«è¦‹ã¤ã‹ã£ãŸæœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
MODEL_NAME = "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"

# 1) ãƒ¢ãƒ‡ãƒ« & ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶
tok = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(CKPT, num_labels=2)

# 2) TSVâ†’Datasetâ†’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºâ†’æ•´å½¢
def load_and_tokenize(split):
    ds = load_dataset("csv", data_files=f"{DATA_DIR}/{split}_binary.tsv", delimiter="\t")["train"]
    # ãƒ©ãƒ™ãƒ«åã‚’ HuggingFace æ¨™æº–ã«
    if "labels" not in ds.column_names and "label" in ds.column_names:
        ds = ds.rename_column("label", "labels")
    # ãƒ©ãƒ™ãƒ«å‹ã‚’ int ã«
    if ds.features["labels"].dtype != "int64":
        ds = ds.map(lambda ex: {"labels": int(ex["labels"])})
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
    ds = ds.map(lambda batch: tok(batch["sentence"], truncation=True, padding="max_length", max_length=512),
                batched=True, num_proc=4)
    # ä½™åˆ†ãªåˆ—ã‚’å‰Šé™¤
    cols_to_drop = [c for c in ["sentence","h","h_type","t","t_type","token_type_ids"] if c in ds.column_names]
    ds = ds.remove_columns(cols_to_drop)
    # Torch å½¢å¼ã¸
    ds = ds.with_format("torch", columns=["input_ids","attention_mask","labels"])
    return ds

dev_ds  = load_and_tokenize("dev")
test_ds = load_and_tokenize("test")

# 3) æŒ‡æ¨™ï¼ˆä»»æ„ï¼šå­¦ç¿’æ™‚ã¨åŒã˜ã‚‚ã®ï¼‰
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average="binary", zero_division=0)
    acc = accuracy_score(labels, preds)
    try:
        auc = roc_auc_score(labels, logits[:, 1])
    except Exception:
        auc = float("nan")
    return {"precision": p, "recall": r, "f1": f1, "accuracy": acc, "auc": auc}

# 4) è©•ä¾¡å°‚ç”¨ã® Trainerï¼ˆtokenizer å¼•æ•°ã¯çœç•¥ã—ã¦OKã€‚è­¦å‘Šã‚‚å‡ºã¾ã›ã‚“ï¼‰
args = TrainingArguments(
    output_dir="/content/tmp_eval",
    per_device_eval_batch_size=128,
    fp16=True,
    dataloader_num_workers=2,
    report_to="none",
)

trainer = Trainer(model=model, args=args, compute_metrics=compute_metrics)

print("Dev:", trainer.evaluate(dev_ds))
print("Test:", trainer.evaluate(test_ds))

"""

Precision/Recall/F1 = 0 â€¦ äºˆæ¸¬ãŒ å…¨éƒ¨0ï¼ˆè² ä¾‹ï¼‰ ã«ãªã£ã¦ã¾ã™ã€‚

Accuracy ~0.72â€“0.75 â€¦ è² ä¾‹ãŒå¤šã„ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ãªã®ã§ã€Œå…¨éƒ¨0ã§ã‚‚ãã“ãã“å½“ãŸã‚‹ã€ã ã‘ã€‚

AUC ~0.57â€“0.59 â€¦ ãƒ©ãƒ³ãƒ€ãƒ (0.5)ã‚ˆã‚Šã¯ãƒã‚·ï¼ã‚¹ã‚³ã‚¢è‡ªä½“ã¯å¤šå°‘ã¯å­¦ç¿’ã§ãã¦ã‚‹ã€‚
ğŸ‘‰ çµè«–ï¼š0/1åˆ¤å®šã®ã—ãã„å€¤(æ—¢å®š=0.5)ãŒå³ã—ã™ãã¦é™½æ€§ã‚’å‡ºã›ã¦ã„ãªã„ã®ãŒåŸå› ã€‚


"""

# ã—ãã„å€¤ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ â€œé™½æ€§ã‚‚å‡ºã™â€ è¨­å®š
# Devã§æœ€é©ãªã—ãã„å€¤â†’Dev/Testã‚’ãã®ã—ãã„å€¤ã§å†è©•ä¾¡
import numpy as np
from sklearn.metrics import precision_recall_curve, precision_recall_fscore_support, accuracy_score, roc_auc_score

# 1) Devã®ã‚¹ã‚³ã‚¢ã‚’å–å¾—
dev_pred = trainer.predict(dev_ds)
dev_scores = dev_pred.predictions[:, 1]
dev_labels = dev_pred.label_ids

# 2) PRæ›²ç·šâ†’F1æœ€å¤§ã®ã—ãã„å€¤ã‚’æ¢ã™
prec, rec, ths = precision_recall_curve(dev_labels, dev_scores)
f1s = 2 * prec * rec / (prec + rec + 1e-12)
best = np.nanargmax(f1s)
best_th = ths[best]
print(f"Best threshold on Dev = {best_th:.3f} | P={prec[best]:.3f}, R={rec[best]:.3f}, F1={f1s[best]:.3f}")

# 3) Devã‚’ãã®ã—ãã„å€¤ã§å†è©•ä¾¡
dev_pred_labels = (dev_scores >= best_th).astype(int)
p,r,f1,_ = precision_recall_fscore_support(dev_labels, dev_pred_labels, average="binary", zero_division=0)
acc = accuracy_score(dev_labels, dev_pred_labels)
auc = roc_auc_score(dev_labels, dev_scores)
print(f"[Dev @ {best_th:.3f}] P={p:.3f} R={r:.3f} F1={f1:.3f} Acc={acc:.3f} AUC={auc:.3f}")

# 4) Testã‚‚åŒã˜ã—ãã„å€¤ã§è©•ä¾¡ï¼ˆéå­¦ç¿’ã‚’é¿ã‘ã‚‹ãŸã‚Devã§æ±ºã‚ãŸé–¾å€¤ã‚’æµç”¨ï¼‰
test_pred = trainer.predict(test_ds)
test_scores = test_pred.predictions[:, 1]
test_labels = test_pred.label_ids

test_pred_labels = (test_scores >= best_th).astype(int)
p,r,f1,_ = precision_recall_fscore_support(test_labels, test_pred_labels, average="binary", zero_division=0)
acc = accuracy_score(test_labels, test_pred_labels)
auc = roc_auc_score(test_labels, test_scores)
print(f"[Test @ {best_th:.3f}] P={p:.3f} R={r:.3f} F1={f1:.3f} Acc={acc:.3f} AUC={auc:.3f}")

"""ã—ãã„å€¤ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ä¸€æ°—ã« Recall ãŒå¾©æ´»

ã„ã¾ã®æ•°å­—ã®æ„å‘³
Dev æœ€é©é–¾å€¤ï¼ˆãƒ­ã‚¸ãƒƒãƒˆï¼‰ = âˆ’0.203 â†’ ç¢ºç‡ã«ã™ã‚‹ã¨ â‰ˆ 0.45ï¼ˆsigmoid(-0.203)ï¼‰ã€‚
ã¤ã¾ã‚Šã€Œ0.5ã‚ˆã‚Šå°‘ã—ç·©ã‚ã€ã«ã™ã‚‹ã¨é™½æ€§ã‚’ã‚ˆãæ‹¾ãˆã‚‹ã€‚

Dev: P=0.307 / R=0.893 / F1=0.457

Test: P=0.272 / R=0.883 / F1=0.416

Accuracy ãŒä¸‹ãŒã£ãŸã®ã¯ä»•æ§˜ï¼šé™½æ€§ã‚’ãŸãã•ã‚“å‡ºã™è¨­å®šã ã‹ã‚‰ã€è² ä¾‹ãŒå¤šã„ãƒ‡ãƒ¼ã‚¿ã§ Acc ã¯è½ã¡ã¾ã™ã€‚

AUC ã¯ 0.57â€“0.59ï¼šã‚¹ã‚³ã‚¢è‡ªä½“ã¯ãƒ©ãƒ³ãƒ€ãƒ ã‚ˆã‚Šä¸Šã€‚ã—ãã„å€¤ã§å®Ÿé‹ç”¨ã®å¥½ã¿ï¼ˆRecallå„ªå…ˆï¼‰ã«å¯„ã›ã‚‹ã®ãŒå¦¥å½“ã€‚

ã™ãã‚„ã‚‹å®Ÿå‹™è¨­å®š
é‹ç”¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«åˆã‚ã›ã¦ threshold ã‚’å›ºå®šï¼š

Recallé‡è¦–ãªã‚‰ä»Šã® logit = âˆ’0.203ï¼ˆpâ‰ˆ0.45ï¼‰ ã‚’æ¡ç”¨

Precision ã‚‚ã†å°‘ã—æ¬²ã—ã„ãªã‚‰ã€Dev ã® PR æ›²ç·šã‹ã‚‰ã€ŒPâ‰¥0.4ã€ç­‰ã®ç‚¹ã‚’é¸ã‚“ã§é–¾å€¤ã‚’å°‘ã—ä¸Šã’ã‚‹
"""

# ä½¿ã„å›ã—ç”¨ã‚¹ãƒ‹ãƒšãƒƒãƒˆ
BEST_LOGIT_TH = -0.203         # Devã§å¾—ãŸå€¤
BEST_PROBA_TH = 1/(1+np.exp(-BEST_LOGIT_TH))  # â‰ˆ 0.45

preds = trainer.predict(test_ds)
scores = preds.predictions[:,1]              # positive ã®ãƒ­ã‚¸ãƒƒãƒˆ
y_hat  = (scores >= BEST_LOGIT_TH).astype(int)
# or ç¢ºç‡ã§ç®¡ç†ã—ãŸã„ãªã‚‰:
# y_hat = (scipy.special.expit(scores) >= BEST_PROBA_TH).astype(int)

"""æ¬¡ã®æ”¹å–„ï¼ˆãƒ¢ãƒ‡ãƒ«å´ï¼‰
ä»Šã¯â€œæ–‡ã¾ã‚‹ã”ã¨512â€ã§ã€h/t ä½ç½®æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã“ã“ã‚’ã„ã˜ã‚‹ã¨PrecisionãŒä¸ŠãŒã‚Šã‚„ã™ã„ã§ã™ã€‚

ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æ˜ç¤ºãƒãƒ¼ã‚­ãƒ³ã‚°
[E1] head [/E1] ... [E2] tail [/E2] ã‚’æ–‡ä¸­ã«æŒ¿å…¥ã—ã¦å†å­¦ç¿’ã€‚

æ–‡ã‚’h/tè¿‘å‚ã«ã‚¯ãƒ­ãƒƒãƒ—
h/t ã®æœ€çŸ­æ–‡ã‚„Â±Næ–‡ã ã‘ã«çµã‚‹ï¼ˆãƒã‚¤ã‚ºæ¸›ï¼†ç²¾åº¦â†‘ï¼‰ã€‚

ä¸å‡è¡¡å¯¾ç­–ã®è¦‹ç›´ã—
pos_weight ã‚’ã‚‚ã†å°‘ã—å¼·ã‚ã‚‹ã€ã‚ã‚‹ã„ã¯è² ä¾‹ã‚’è»½ããƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒ«ã€‚

ãƒã‚¤ãƒ‘ãƒ©
A100ãªã‚‰ batch=32, accum=1, epoch=3ã€LR 1e-5 ãªã©ã‚‚è©¦ã™ä¾¡å€¤ã‚ã‚Šã€‚

çµè«–ï¼šã—ãã„å€¤ 0.45 ç›¸å½“ã§ Recall ã‚’ç¢ºä¿ã§ãã¾ã—ãŸã€‚
ã“ã®è¨­å®šã§ä¸€æ—¦ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å›ã—ã¤ã¤ã€ä¸Šã® 1)â€“2)ï¼ˆã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãƒãƒ¼ã‚­ãƒ³ã‚°ï¼‹ã‚¯ãƒ­ãƒƒãƒ—ï¼‰ã‚’å…¥ã‚Œã¦å†å­¦ç¿’ã™ã‚Œã°ã€Precision ã‚’å¼•ãä¸Šã’ã¤ã¤ F1 ã‚’ã•ã‚‰ã«ä¼¸ã°ã›ã‚‹ã¯ãšã§ã™ã€‚å¿…è¦ãªã‚‰ã€ãã®å‰å‡¦ç†ã‚³ãƒ¼ãƒ‰ã‚‚ã™ãæ›¸ãã¾ã™ã€‚
"""

# ã—ãã„å€¤ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„ PR ã‚«ãƒ¼ãƒ–ã‚’æãå ´åˆ
from sklearn.metrics import precision_recall_curve
preds = trainer.predict(dev_ds)
scores = preds.predictions[:, 1]
labels = preds.label_ids
prec, rec, ths = precision_recall_curve(labels, scores)

# Dev ã®ã‚¹ã‚³ã‚¢ã‹ã‚‰ PR æ›²ç·šã‚’ä½œã‚‹ä¸‹æº–å‚™
# æœ€é©ãªã—ãã„å€¤ã‚’æ±‚ã‚ã‚‹â†’å¯è¦–åŒ–â†’ãã®é–¾å€¤ã§Dev/Testã‚’å†è©•ä¾¡
# trainer ã¨ dev_ds/test_ds ãŒç”¨æ„æ¸ˆã¿å‰æ

# === Devã§PRæ›²ç·š â†’ æœ€é©ã—ãã„å€¤æ±ºå®š â†’ å¯è¦–åŒ– â†’ Dev/Testå†è©•ä¾¡ ä¸€æ‹¬ã‚»ãƒ« ===
import numpy as np, matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, precision_recall_fscore_support, accuracy_score, roc_auc_score, average_precision_score

# 1) Devã®ã‚¹ã‚³ã‚¢å–å¾—ï¼ˆâ€»scoresã¯positiveã‚¯ãƒ©ã‚¹ã®ãƒ­ã‚¸ãƒƒãƒˆæƒ³å®šï¼‰
preds = trainer.predict(dev_ds)
scores = preds.predictions[:, 1]
labels = preds.label_ids

# 2) PRæ›²ç·šã¨å¹³å‡é©åˆç‡ï¼ˆAPï¼‰
prec, rec, ths = precision_recall_curve(labels, scores)
ap = average_precision_score(labels, scores)

# 3) F1æœ€å¤§ã®ã—ãã„å€¤ã‚’é¸ã¶
f1s = 2 * prec * rec / (prec + rec + 1e-12)
best = np.nanargmax(f1s)
best_th = ths[best]                      # â† ãƒ­ã‚¸ãƒƒãƒˆã®é–¾å€¤
best_p, best_r, best_f1 = prec[best], rec[best], f1s[best]
print(f"[Dev] Best logit threshold = {best_th:.3f} | P={best_p:.3f} R={best_r:.3f} F1={best_f1:.3f} | AP={ap:.3f}")

# 4) å¯è¦–åŒ–ï¼ˆãƒ™ã‚¹ãƒˆç‚¹ã‚’ãƒãƒ¼ã‚­ãƒ³ã‚°ï¼‰
plt.figure()
plt.plot(rec, prec, label=f"PR curve (AP={ap:.3f})")
plt.scatter([best_r], [best_p], s=60, marker='o', label=f"Best F1@{best_th:.3f}")
plt.xlabel("Recall"); plt.ylabel("Precision"); plt.title("Precisionâ€“Recall (Dev)")
plt.grid(True); plt.legend(); plt.show()

# 5) Devã‚’ãƒ™ã‚¹ãƒˆé–¾å€¤ã§å†è©•ä¾¡
y_hat_dev = (scores >= best_th).astype(int)
p,r,f1,_ = precision_recall_fscore_support(labels, y_hat_dev, average="binary", zero_division=0)
acc = accuracy_score(labels, y_hat_dev); auc = roc_auc_score(labels, scores)
print(f"[Dev @ {best_th:.3f}] P={p:.3f} R={r:.3f} F1={f1:.3f} Acc={acc:.3f} AUC={auc:.3f}")

# 6) Testã«ã‚‚åŒã˜é–¾å€¤ã‚’é©ç”¨ï¼ˆéå­¦ç¿’å›é¿ã®åŸºæœ¬ï¼‰
test_pred = trainer.predict(test_ds)
test_scores = test_pred.predictions[:, 1]
test_labels = test_pred.label_ids
y_hat_test = (test_scores >= best_th).astype(int)
p,r,f1,_ = precision_recall_fscore_support(test_labels, y_hat_test, average="binary", zero_division=0)
acc = accuracy_score(test_labels, y_hat_test); auc = roc_auc_score(test_labels, test_scores)
print(f"[Test @ {best_th:.3f}] P={p:.3f} R={r:.3f} F1={f1:.3f} Acc={acc:.3f} AUC={auc:.3f}")

# ï¼ˆå¿…è¦ãªã‚‰ï¼‰å›³ã‚’Driveã«ä¿å­˜
# import os
# out_png = "/content/drive/MyDrive/BioRED/pr_curve_dev.png"
# plt.figure(); plt.plot(rec, prec); plt.scatter([best_r],[best_p],s=60); plt.xlabel("Recall"); plt.ylabel("Precision"); plt.grid(True); plt.savefig(out_png, dpi=150); print("Saved:", out_png)

"""ã‚°ãƒ©ãƒ•ã„ã„ã€‚èª­ã¿å–ã‚Šã¨ã—ã¦ã¯ï¼š

AP=0.319 â†’ ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã§ã¯ ROC ã‚ˆã‚Š PR ãŒé‡è¦ã€‚ãƒ©ãƒ³ãƒ€ãƒ (â‰’é™½æ€§ç‡)ã‚ˆã‚Šä¸Šã§ã€å­¦ç¿’ã¯ã§ãã¦ã¾ã™ãŒã€ã¾ã ä½™åœ°ã‚ã‚Šã€‚

æ›²ç·šã¯ Pâ‰ˆ0.3 ã§é•·ããƒ•ãƒ©ãƒƒãƒˆã€å³ç«¯ï¼ˆé«˜ Recallï¼‰ã¾ã§ç¶šã„ã¦ã„ã¾ã™ï¼é–¾å€¤ã‚’ä¸‹ã’ã‚Œã°é™½æ€§ã‚’ã‚ˆãæ‹¾ãˆã‚‹ã‚¿ã‚¤ãƒ—ã€‚

ãƒ™ã‚¹ãƒˆF1ã¯ logit = âˆ’0.203ï¼ˆç¢ºç‡â‰ˆ0.45ï¼‰ ä»˜è¿‘ï¼šRecall â‰ˆ 0.89 / Precision â‰ˆ 0.31ã€‚
ç›®çš„ãŒã€Œè¦‹è½ã¨ã—ã‚’æ¸›ã‚‰ã™ï¼ˆRecallé‡è¦–ï¼‰ã€ãªã‚‰å¦¥å½“ãªå‹•ä½œç‚¹ã§ã™ã€‚
"""

# 1) é–¾å€¤ã‚’ç¢ºå®šã—ã¦ã€Dev/Test ã‚’å†è©•ä¾¡ï¼ˆè¡¨ã§ç¢ºèªï¼‰
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score

def eval_at_threshold(scores, labels, th):
    yhat = (scores >= th).astype(int)
    p,r,f1,_ = precision_recall_fscore_support(labels, yhat, average="binary", zero_division=0)
    acc = accuracy_score(labels, yhat)
    return p,r,f1,acc

# ã—ãã„å€¤å€™è£œï¼ˆç¢ºç‡ã§ã¯ãªããƒ­ã‚¸ãƒƒãƒˆå‰æï¼‰
candidates = np.linspace(-1.0, 0.5, 13)  # ãŠå¥½ã¿ã§å¤‰æ›´
dev_pred = trainer.predict(dev_ds); dev_scores = dev_pred.predictions[:,1]; dev_labels = dev_pred.label_ids

rows = []
for th in candidates:
    p,r,f1,acc = eval_at_threshold(dev_scores, dev_labels, th)
    rows.append((th, p, r, f1, acc))
for th,p,r,f1,acc in rows:
    print(f"th={th:+.3f} | P={p:.3f} R={r:.3f} F1={f1:.3f} Acc={acc:.3f}")

"""ã„ã„ãƒ‡ãƒ¼ã‚¿ã€‚è¡¨ã®èª­ã¿æ–¹ã¨çµè«–ã€‚

ä½•ãŒåˆ†ã‹ã‚‹ï¼Ÿ
ã—ãã„å€¤ã‚’ä¸‹ã’ã‚‹ã»ã©ï¼ˆâˆ’1.0 â†’ 0.0ï¼‰Recall ã¯ä¸ŠãŒã‚‹ï¼Accuracy ã¯ä¸‹ãŒã‚‹ã€‚

F1 ã®å±±ã¯ âˆ’0.25 ä»˜è¿‘ï¼ˆ0.449ï¼‰ã€‚å…ˆã»ã© PR ã‹ã‚‰æ±‚ã‚ãŸ âˆ’0.203ï¼ˆF1=0.457ï¼‰ ã®ã»ã†ãŒåƒ…ã‹ã«è‰¯ã„ã§ã™ã€‚

ã¤ã¾ã‚Š ã€ŒDev ã§æœ€é©F1ã€ã‚’ç‹™ã†ãªã‚‰ âˆ’0.203 ã‚’æ¡ç”¨ã€å››æ¨äº”å…¥ã—ã¦æ‰±ã„ã‚„ã™ãã™ã‚‹ãªã‚‰ âˆ’0.25 ã§ã‚‚ã»ã¼åŒç­‰ã§ã™ã€‚

ç›®çš„ãŒã€Œå› æœæ–‡ãƒ»ãƒˆãƒªãƒ—ãƒ«ã‚’è¦‹è½ã¨ã—ãŸããªã„ï¼ˆRecallé‡è¦–ï¼‰ã€ãªã‚‰ã€
âˆ’0.25ã€œâˆ’0.20 ã®ã‚¾ãƒ¼ãƒ³ãŒå¦¥å½“ã§ã™ï¼ˆRecall ~0.9ã€Precision ~0.3ï¼‰ã€‚

ãŠã™ã™ã‚é‹ç”¨
Dev ã§å¾—ãŸæœ€é©ï¼ˆâˆ’0.203ï¼‰ã‚’æ¡ç”¨ â†’ Test ã«ãã®ã¾ã¾é©ç”¨

ã‚‚ã—ã€ŒPrecision ã‚’ã‚‚ã†å°‘ã—ä¸Šã’ãŸã„ã€è¦ä»¶ãŒå‡ºãŸã‚‰ã€âˆ’0.125ã€œ0.0 ã«ä¸Šã’ã‚‹ï¼ˆF1ã¯ã‚„ã‚„ä¸‹ã‚‹ãŒAccã¯ä¸ŠãŒã‚‹ï¼‰
"""

# 2) é¸ã‚“ã é–¾å€¤ã§ Test ã‚’å›ºå®šè©•ä¾¡ & ä¿å­˜
import pandas as pd
from scipy.special import expit

BEST_LOGIT_TH = -0.203  # â† Devã§æ±ºã‚ãŸå€¤ã‚’å…¥ã‚Œã‚‹
# Testã‚¹ã‚³ã‚¢
test_pred = trainer.predict(test_ds)
test_scores = test_pred.predictions[:,1]
test_labels = test_pred.label_ids
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹
p,r,f1,_ = precision_recall_fscore_support(test_labels, (test_scores>=BEST_LOGIT_TH).astype(int), average="binary", zero_division=0)
acc = accuracy_score(test_labels, (test_scores>=BEST_LOGIT_TH).astype(int))
auc = roc_auc_score(test_labels, test_scores)
print(f"[Test @{BEST_LOGIT_TH:+.3f}] P={p:.3f} R={r:.3f} F1={f1:.3f} Acc={acc:.3f} AUC={auc:.3f}")

# äºˆæ¸¬ã®æ›¸ãå‡ºã—ï¼ˆæ–‡ãƒ»h/tä»˜ãã§è¦‹ãŸã„å ´åˆï¼‰
DATA_DIR = "/content/drive/MyDrive/BioRED"
raw = pd.read_csv(f"{DATA_DIR}/test_binary.tsv", sep="\t")  # å…ƒã®é †åºã¯ä¿æŒã•ã‚Œã¾ã™
raw["logit"] = test_scores
raw["proba"] = expit(test_scores)
raw["pred"]  = (test_scores >= BEST_LOGIT_TH).astype(int)
raw.to_csv(f"{DATA_DIR}/test_pred_pubmedbert.csv", index=False)
print("Saved:", f"{DATA_DIR}/test_pred_pubmedbert.csv")

"""æ¬¡ã®ä¸€æ‰‹ï¼ˆç²¾åº¦ã‚’ã•ã‚‰ã«ä¸Šã’ãŸã„ãªã‚‰ï¼‰
ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®æ˜ç¤ºãƒãƒ¼ã‚­ãƒ³ã‚°ï¼š[E1] h [/E1] ... [E2] t [/E2] ã‚’æ–‡ã«æŒ¿å…¥ã—ã¦å†å­¦ç¿’ï¼ˆPrecisionãŒä¸ŠãŒã‚Šã‚„ã™ã„ï¼‰ã€‚

h/t è¿‘å‚ã«ã‚¯ãƒ­ãƒƒãƒ—ï¼šå¯¾è±¡æ–‡ã‚„å‰å¾ŒÂ±1æ–‡ã ã‘ã«çµã‚‹ã€‚

ä¸å‡è¡¡å¯¾ç­–ï¼špos_weight ã‚’å°‘ã—å¼·ã‚ã‚‹ or è² ä¾‹ã‚’è»½ããƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒ«ã€‚

A100å‰æã®å†å­¦ç¿’ï¼šbatch=32, accum=1, epoch=3, lr=1e-5 ã‚’è©¦ã™ã€‚

ã“ã®ã¾ã¾ã€Œâˆ’0.203 å›ºå®šã€ã§å›ã™ã‹ã€ã€Œâˆ’0.25ã€ã«ä¸¸ã‚ã‚‹ã‹ã€ã©ã¡ã‚‰ã«ã—ã¾ã™ï¼ŸåŸºæº–ï¼ˆRecallå„ªå…ˆ or Precisionå°‘ã—ä¸Šã’ã‚‹ï¼‰ã‚’æ•™ãˆã¦ãã‚ŒãŸã‚‰æœ€é©ç‚¹ã‚’æ±ºã‚ã¾ã™ã€‚
"""

# 3) ã‚µãƒƒã¨ã‚¨ãƒ©ãƒ¼åˆ†æï¼ˆFP/FN ã‚’è¦—ãï¼‰
fp = raw[(raw["label"]==0) & (raw["pred"]==1)].sample(20, random_state=0)
fn = raw[(raw["label"]==1) & (raw["pred"]==0)].sample(20, random_state=0)
fp[["sentence","h","t","proba"]].head(3), fn[["sentence","h","t","proba"]].head(3)

# FPãŒå¤šã„ãªã‚‰ï¼šPrecision ã‚’ä¸Šã’ãŸã„ â†’ é–¾å€¤â†‘ or å‰å‡¦ç†æ”¹å–„
# FNãŒå¤šã„ãªã‚‰ï¼šRecall ã‚’ä¸Šã’ãŸã„ â†’ é–¾å€¤â†“ or ãƒ¢ãƒ‡ãƒ«å¼·åŒ–

"""å‡ºåŠ›ã¯ãŠãã‚‰ã

å·¦ï¼šFPï¼ˆå½é™½æ€§ï¼‰â€¦pred=1 ã ã‘ã© label=0ã€proba ~0.50â€“0.58ï¼ˆå¢ƒç•Œä»˜è¿‘ï¼‰

å³ï¼šFNï¼ˆå½é™°æ€§ï¼‰â€¦pred=0 ã ã‘ã© label=1ã€proba ~0.41â€“0.43ï¼ˆã‚„ã¯ã‚Šå¢ƒç•Œä»˜è¿‘ï¼‰

ã“ã“ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹ã“ã¨
ã©ã¡ã‚‰ã‚‚ã—ãã„å€¤ä»˜è¿‘ã®æ›–æ˜§ã‚µãƒ³ãƒ—ãƒ«ãŒå¤šã„ â†’ ã—ãã„å€¤èª¿æ•´ã§è£è¿”ã‚‹ã‚¿ã‚¤ãƒ—ã€‚

FPã®ä¾‹ã« â€œpatientsâ€ ã®ã‚ˆã†ãªæ±ç”¨åè©ãŒæ··ã˜ã‚‹ â†’ ãƒã‚¤ã‚ºæºã«ãªã‚Šã‚„ã™ã„ã€‚

é•·æ–‡ãƒ»ç›¸é–¢èªï¼ˆassociationç­‰ï¼‰ã§å› æœã¨ç›¸é–¢ã®æ··åŒãŒèµ·ããŒã¡ã€‚



ã™ãåŠ¹ãå¯¾ç­–ï¼ˆå­¦ç¿’ã—ç›´ã—ä¸è¦ï¼‰
A) ãƒ«ãƒ¼ãƒ«ç³»ã®å¾Œæ®µãƒ•ã‚£ãƒ«ã‚¿ï¼ˆPrecisionâ†‘ï¼‰
æ±ç”¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®é™¤å¤–ï¼š{'patients','subjects','men','women','children'} ç­‰ã‚’ h/t ã«å«ã‚€äºˆæ¸¬ã¯å¼¾ã

å› æœã‚­ãƒ¥ãƒ¼èªã®ãƒ–ãƒ¼ã‚¹ãƒˆï¼šæ–‡ã« {'cause','induce','lead to','increase','decrease','due to'} ãªã©ãŒç„¡ã„å ´åˆã¯ã€ã—ãã„å€¤ã‚’ 0.05 å¼•ãä¸Šã’ã‚‹ ç­‰
"""

# å®Ÿè¡Œã‚¹ãƒ‹ãƒšãƒƒãƒˆï¼ˆç¢ºå®šã—ãã„å€¤ã§ Test ã‚’è©•ä¾¡ï¼†ä¿å­˜ï¼‰
import numpy as np, pandas as pd
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score
from scipy.special import expit

BEST_LOGIT_TH = -0.203   # ã¾ãŸã¯ -0.25 ã«å·®ã—æ›¿ãˆ

# Dev/Test ã‚¹ã‚³ã‚¢å–å¾—ï¼ˆã™ã§ã« trainer ãŒã‚ã‚‹å‰æï¼‰
dev_pred = trainer.predict(dev_ds)
test_pred = trainer.predict(test_ds)
dev_scores, dev_labels   = dev_pred.predictions[:,1], dev_pred.label_ids
test_scores, test_labels = test_pred.predictions[:,1], test_pred.label_ids

def eval_at(scores, labels, th):
    yhat = (scores >= th).astype(int)
    p,r,f1,_ = precision_recall_fscore_support(labels, yhat, average="binary", zero_division=0)
    acc = accuracy_score(labels, yhat)
    auc = roc_auc_score(labels, scores)
    return p,r,f1,acc,auc

print("[Dev]",  eval_at(dev_scores,  dev_labels,  BEST_LOGIT_TH))
print("[Test]", eval_at(test_scores, test_labels, BEST_LOGIT_TH))

# äºˆæ¸¬ã‚’æ›¸ãå‡ºã—ï¼ˆåˆ†æç”¨ï¼‰
DATA_DIR = "/content/drive/MyDrive/BioRED"
raw = pd.read_csv(f"{DATA_DIR}/test_binary.tsv", sep="\t")
raw["logit"] = test_scores
raw["proba"] = expit(test_scores)
raw["pred"]  = (test_scores >= BEST_LOGIT_TH).astype(int)
raw.to_csv(f"{DATA_DIR}/test_pred_pubmedbert@{BEST_LOGIT_TH:+.3f}.csv", index=False)
print("Saved:", f"{DATA_DIR}/test_pred_pubmedbert@{BEST_LOGIT_TH:+.3f}.csv")

"""æ¬¡ã®ä¸€æ‰‹ï¼ˆç²¾åº¦ã‚’ã•ã‚‰ã«ä¸Šã’ãŸã„ãªã‚‰ï¼‰
ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®æ˜ç¤ºãƒãƒ¼ã‚­ãƒ³ã‚°ï¼š[E1] h [/E1] ... [E2] t [/E2] ã‚’æ–‡ã«æŒ¿å…¥ã—ã¦å†å­¦ç¿’ï¼ˆPrecisionãŒä¸ŠãŒã‚Šã‚„ã™ã„ï¼‰ã€‚

h/t è¿‘å‚ã«ã‚¯ãƒ­ãƒƒãƒ—ï¼šå¯¾è±¡æ–‡ã‚„å‰å¾ŒÂ±1æ–‡ã ã‘ã«çµã‚‹ã€‚

ä¸å‡è¡¡å¯¾ç­–ï¼špos_weight ã‚’å°‘ã—å¼·ã‚ã‚‹ or è² ä¾‹ã‚’è»½ããƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒ«ã€‚

A100å‰æã®å†å­¦ç¿’ï¼šbatch=32, accum=1, epoch=3, lr=1e-5 ã‚’è©¦ã™ã€‚

ã“ã®ã¾ã¾ã€Œâˆ’0.203 å›ºå®šã€ã§å›ã™ã‹ã€ã€Œâˆ’0.25ã€ã«ä¸¸ã‚ã‚‹ã‹ã€ï¼ŸåŸºæº–ï¼ˆRecallå„ªå…ˆ or Precisionå°‘ã—ä¸Šã’ã‚‹ï¼‰=>æœ€é©ç‚¹ã‚’æ±ºã‚ã¾ã™ã€‚

ç‹™ã„ã©ãŠã‚Š Recall ã‚’å¤§ããç¢ºä¿ï¼ˆ~0.88ï¼‰ã€‚Accuracy ãŒä¸‹ãŒã£ã¦ã„ã‚‹ã®ã¯ã€é™½æ€§ã‚’å¤šã‚ã«å‡ºã™é–¾å€¤ã«ã—ãŸçµæœãªã®ã§æƒ³å®šå†…ã€‚AUC ã¯å­¦ç¿’æ™‚ã¨åŒç¨‹åº¦ã§ã€ã—ãã„å€¤èª¿æ•´ã§é‹ç”¨æŒ‡æ¨™ï¼ˆF1/Recallï¼‰ã‚’æ”¹å–„ã§ããŸå½¢ã§ã™ã€‚
"""

# CKPTã‚’èª­ã¿ç›´ã—ãŸå¾Œã« Drive ã¸é€€é¿
import os, json
SAVE_DIR = "/content/drive/MyDrive/BioRED/biored_pubmedbert_bin_ckpt15708"
os.makedirs(SAVE_DIR, exist_ok=True)

model.save_pretrained(SAVE_DIR)
tok.save_pretrained(SAVE_DIR)

# ã—ãã„å€¤ã‚’ä¿å­˜ï¼ˆä»»æ„ï¼‰
with open(os.path.join(SAVE_DIR, "threshold.json"), "w") as f:
    json.dump({"best_logit_th": float(best_th)}, f)

print("Saved to:", SAVE_DIR)

"""è£œè¶³ï¼šå­¦ç¿’å†é–‹ã‚’è¦‹è¶Šã™ãªã‚‰ trainer.save_state() ã‚‚ä½µã›ã¦ä¿å­˜ã—ã¦ãŠãã¨ã€
trainer.train(resume_from_checkpoint=True) ã§ ç¶šãã‹ã‚‰å†é–‹ã§ãã¾ã™ã€‚
"""

# PRæ›²ç·šã®æç”»ã¯å¯è¦–åŒ–ãƒ»ç¢ºèªç”¨ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€‚å¿…é ˆã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
from sklearn.metrics import precision_recall_curve
import numpy as np

preds = trainer.predict(dev_ds)
scores = preds.predictions[:, 1]   # positive ã®ã‚¹ã‚³ã‚¢ï¼ˆãƒ­ã‚¸ãƒƒãƒˆã§OKï¼‰
labels = preds.label_ids

prec, rec, ths = precision_recall_curve(labels, scores)
f1 = 2 * prec * rec / (prec + rec + 1e-12)
best = np.nanargmax(f1)
best_th = ths[best]
print(f"Best threshold (logit) = {best_th:.3f} | P={prec[best]:.3f} R={rec[best]:.3f} F1={f1[best]:.3f}")

# æ±ºã‚ãŸã—ãã„å€¤ã ã‘ä¿å­˜ã™ã‚‹å ´åˆ
import json, os
SAVE_DIR = "/content/drive/MyDrive/BioRED/models/pubmedbert_bin_best"
os.makedirs(SAVE_DIR, exist_ok=True)
with open(os.path.join(SAVE_DIR, "threshold.json"), "w") as f:
    json.dump({"best_logit_th": float(best_th)}, f)

# ä¸Šã®ç¶šãã€å¾©å¸°æ™‚ã¯ã“ã†èª­ã‚€
import json, os
with open(os.path.join(SAVE_DIR, "threshold.json")) as f:
    best_th = json.load(f)["best_logit_th"]

# ä¸è¦ã€PRæ›²ç·šã®æç”»ã‚»ãƒ«ã§å¯è¦–åŒ–ç”¨
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt

preds = trainer.predict(dev_ds)
scores = preds.predictions[:, 1]
labels = preds.label_ids

prec, rec, ths = precision_recall_curve(labels, scores)
plt.figure()
plt.plot(rec, prec)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precisionâ€“Recall Curve on Dev Set')
plt.grid(True)
plt.show()