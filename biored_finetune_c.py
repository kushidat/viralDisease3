# -*- coding: utf-8 -*-
"""biored_finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ezTeh2f0b7Jr_70awRWJkTNhBmnsu6Be
"""

# biored_finetune.py
"""
End‑to‑end Colab script for fine‑tuning PubMedBERT on the BioRED binary‑relation task
using memory‑saving settings that work in a free Colab GPU session (≈12 GB RAM).

✓ mounts Drive
✓ installs nothing (assumes runtime already has transformers ≥ 4.55, datasets ≥ 2.19)
✓ tokenises train/dev/test TSVs (train_binary.tsv etc.)
✓ removes unused columns & switches to lazy Torch format
✓ computes pos_weight for class‑imbalance
✓ trains with small batch 4 + grad‑accum 4 (effective 16)
✓ evaluates & runs a demo inference
"""

import os, numpy as np, torch, torch.nn as nn
from google.colab import drive
from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer,
)
from sklearn.metrics import (
    precision_recall_fscore_support,
    accuracy_score,
    roc_auc_score,
)

# ---------------------------- 0. Mount Drive ----------------------------
drive.mount("/content/drive")

# ---------------------------- 1. Paths & constants ----------------------
DATA_DIR = "/content/drive/MyDrive/BioRED"  # <-- adjust if needed
MODEL_NAME = "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"
BATCH_SIZE = 32
GRAD_ACCUM = 1
EPOCHS = 2

# ---------------------------- 2. Load TSVs ------------------------------

def load_split(split: str):
    path = os.path.join(DATA_DIR, f"{split}_binary.tsv")
    return load_dataset("csv", data_files=path, delimiter="\t")['train']

train_ds = load_split("train")
dev_ds   = load_split("dev")
test_ds  = load_split("test")
print("Loaded", len(train_ds), "train /", len(dev_ds), "dev /", len(test_ds), "test")

# ---------------------------- 3. Tokenisation ---------------------------
tok = AutoTokenizer.from_pretrained(MODEL_NAME)

def tokenize(batch):
    return tok(batch["sentence"], truncation=True, padding="max_length", max_length=512)

for name, ds in [("train", train_ds), ("dev", dev_ds), ("test", test_ds)]:
    if "sentence" in ds.column_names:      # test might already be tokenised in older runs
        ds = ds.map(tokenize, batched=True, num_proc=4 if name=="train" else 2)
    if "label" in ds.column_names:
        ds = ds.rename_column("label", "labels")
    # remove unused text / entity columns if still present
    cols_to_drop = [c for c in ['sentence','h','h_type','t','t_type','token_type_ids'] if c in ds.column_names]
    ds = ds.remove_columns(cols_to_drop)
    # keep as lazy Torch tensors → huge RAM saving
    ds.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
    if name=="train": train_ds=ds
    elif name=="dev": dev_ds=ds
    else: test_ds=ds
print("Final columns:", train_ds.column_names)

# ---------------------------- 4. pos_weight -----------------------------
pos = torch.tensor(train_ds["labels"]).sum()
neg = len(train_ds) - pos
pos_weight = torch.tensor([neg/pos])
print("pos_weight =", pos_weight.item())

# ---------------------------- 5. Model ----------------------------------
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)

# ---------------------------- 6. Custom Trainer -------------------------
class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **_):
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(outputs.logits.device))
        loss = loss_fn(outputs.logits, torch.nn.functional.one_hot(labels,2).float())
        return (loss, outputs) if return_outputs else loss

def compute_metrics(pred):
    logits, labels = pred
    preds = np.argmax(logits, axis=-1)
    p,r,f1,_ = precision_recall_fscore_support(labels, preds, average="binary")
    acc = accuracy_score(labels, preds)
    auc = roc_auc_score(labels, logits[:,1])
    return {"precision":p, "recall":r, "f1":f1, "accuracy":acc, "auc":auc}

# ---------------------------- 7. TrainingArguments ----------------------
args = TrainingArguments(
    output_dir="biored_pubmedbert_bin",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=32,
    gradient_accumulation_steps=1,
    num_train_epochs=EPOCHS,
    fp16=True,
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to="none",   # no W&B
)

trainer = WeightedTrainer(
    model=model,
    args=args,
    train_dataset=train_ds,
    eval_dataset=dev_ds,
    compute_metrics=compute_metrics,
)

# ---------------------------- 8. Train & evaluate -----------------------
trainer.train()
print("Dev metrics:", trainer.evaluate())
print("Test metrics:", trainer.evaluate(test_ds))

# 学習直後に保存（推奨）
import os, json
SAVE_DIR = "/content/drive/MyDrive/BioRED/models/pubmedbert_bin_best"
os.makedirs(SAVE_DIR, exist_ok=True)

# ベストモデルを Drive に保存
trainer.save_model(SAVE_DIR)     # ← model と config を保存
tok.save_pretrained(SAVE_DIR)    # ← tokenizer も同じ場所に保存

# 閾値もメモしておく（例：Devで求めた best_th）
with open(os.path.join(SAVE_DIR, "threshold.json"), "w") as f:
    json.dump({"best_logit_th": float(best_th)}, f)

print("Saved to:", SAVE_DIR)

"""補足：学習再開を見越すなら trainer.save_state() も併せて保存しておくと、
trainer.train(resume_from_checkpoint=True) で 続きから再開できます。
"""

# モデルと入力テンソルを両方とも同じデバイス（GPU なら cuda）に載せるように修正
import torch

# ① デバイスを取得してモデルを移動
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# ② トークナイズ→GPU に転送
example = "IL-6 causes fever in inflammatory responses."
inputs = tok(
    example,
    return_tensors="pt",
    truncation=True,
    max_length=512
)
# all tensors to same device
inputs = {k: v.to(device) for k, v in inputs.items()}

# ③ 推論＆確率計算
with torch.no_grad():
    logits = model(**inputs).logits
    proba  = torch.softmax(logits, dim=1)[0, 1].item()

print(f"Relation probability: {proba:.3f}")

# Dev でのすべての予測スコアを取得
preds = trainer.predict(dev_ds)
scores = preds.predictions[:,1]
labels = preds.label_ids
# sklearn などで precision_recall_curve を描く
from sklearn.metrics import precision_recall_curve
prec, rec, ths = precision_recall_curve(labels, scores)

# 学習済みモデルとデータセットを読み込む
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer

DATA_DIR = "/content/drive/MyDrive/BioRED"
dev_ds = load_dataset("csv", data_files=f"{DATA_DIR}/dev_binary.tsv", delimiter="\t")["train"]
# 既存のトークナイズ＆整形関数をここに再適用

# config.json と pytorch_model.bin（または model.safetensors）の確認
import os, glob
for p in [
    "/content/drive/MyDrive/BioRED/biored_pubmedbert_bin",
    "/content/biored_pubmedbert_bin"
]:
    print("DIR?", p, os.path.isdir(p))
    if os.path.isdir(p):
        print("FILES:", [os.path.basename(x) for x in glob.glob(os.path.join(p, "*"))][:10])

# 学習はやり直さず、「Dev/Test を読み込み→トークナイズ→評価」だけやり直し
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score

DATA_DIR = "/content/drive/MyDrive/BioRED"
CKPT = "/content/biored_pubmedbert_bin/checkpoint-15708"  # 直前に見つかった最新チェックポイント
MODEL_NAME = "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract"

# 1) モデル & トークナイザ
tok = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(CKPT, num_labels=2)

# 2) TSV→Dataset→トークナイズ→整形
def load_and_tokenize(split):
    ds = load_dataset("csv", data_files=f"{DATA_DIR}/{split}_binary.tsv", delimiter="\t")["train"]
    # ラベル名を HuggingFace 標準に
    if "labels" not in ds.column_names and "label" in ds.column_names:
        ds = ds.rename_column("label", "labels")
    # ラベル型を int に
    if ds.features["labels"].dtype != "int64":
        ds = ds.map(lambda ex: {"labels": int(ex["labels"])})
    # トークナイズ
    ds = ds.map(lambda batch: tok(batch["sentence"], truncation=True, padding="max_length", max_length=512),
                batched=True, num_proc=4)
    # 余分な列を削除
    cols_to_drop = [c for c in ["sentence","h","h_type","t","t_type","token_type_ids"] if c in ds.column_names]
    ds = ds.remove_columns(cols_to_drop)
    # Torch 形式へ
    ds = ds.with_format("torch", columns=["input_ids","attention_mask","labels"])
    return ds

dev_ds  = load_and_tokenize("dev")
test_ds = load_and_tokenize("test")

# 3) 指標（任意：学習時と同じもの）
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average="binary", zero_division=0)
    acc = accuracy_score(labels, preds)
    try:
        auc = roc_auc_score(labels, logits[:, 1])
    except Exception:
        auc = float("nan")
    return {"precision": p, "recall": r, "f1": f1, "accuracy": acc, "auc": auc}

# 4) 評価専用の Trainer（tokenizer 引数は省略してOK。警告も出ません）
args = TrainingArguments(
    output_dir="/content/tmp_eval",
    per_device_eval_batch_size=128,
    fp16=True,
    dataloader_num_workers=2,
    report_to="none",
)

trainer = Trainer(model=model, args=args, compute_metrics=compute_metrics)

print("Dev:", trainer.evaluate(dev_ds))
print("Test:", trainer.evaluate(test_ds))

"""

Precision/Recall/F1 = 0 … 予測が 全部0（負例） になってます。

Accuracy ~0.72–0.75 … 負例が多い不均衡データなので「全部0でもそこそこ当たる」だけ。

AUC ~0.57–0.59 … ランダム(0.5)よりはマシ＝スコア自体は多少は学習できてる。
👉 結論：0/1判定のしきい値(既定=0.5)が厳しすぎて陽性を出せていないのが原因。


"""

# しきい値チューニングで “陽性も出す” 設定
# Devで最適なしきい値→Dev/Testをそのしきい値で再評価
import numpy as np
from sklearn.metrics import precision_recall_curve, precision_recall_fscore_support, accuracy_score, roc_auc_score

# 1) Devのスコアを取得
dev_pred = trainer.predict(dev_ds)
dev_scores = dev_pred.predictions[:, 1]
dev_labels = dev_pred.label_ids

# 2) PR曲線→F1最大のしきい値を探す
prec, rec, ths = precision_recall_curve(dev_labels, dev_scores)
f1s = 2 * prec * rec / (prec + rec + 1e-12)
best = np.nanargmax(f1s)
best_th = ths[best]
print(f"Best threshold on Dev = {best_th:.3f} | P={prec[best]:.3f}, R={rec[best]:.3f}, F1={f1s[best]:.3f}")

# 3) Devをそのしきい値で再評価
dev_pred_labels = (dev_scores >= best_th).astype(int)
p,r,f1,_ = precision_recall_fscore_support(dev_labels, dev_pred_labels, average="binary", zero_division=0)
acc = accuracy_score(dev_labels, dev_pred_labels)
auc = roc_auc_score(dev_labels, dev_scores)
print(f"[Dev @ {best_th:.3f}] P={p:.3f} R={r:.3f} F1={f1:.3f} Acc={acc:.3f} AUC={auc:.3f}")

# 4) Testも同じしきい値で評価（過学習を避けるためDevで決めた閾値を流用）
test_pred = trainer.predict(test_ds)
test_scores = test_pred.predictions[:, 1]
test_labels = test_pred.label_ids

test_pred_labels = (test_scores >= best_th).astype(int)
p,r,f1,_ = precision_recall_fscore_support(test_labels, test_pred_labels, average="binary", zero_division=0)
acc = accuracy_score(test_labels, test_pred_labels)
auc = roc_auc_score(test_labels, test_scores)
print(f"[Test @ {best_th:.3f}] P={p:.3f} R={r:.3f} F1={f1:.3f} Acc={acc:.3f} AUC={auc:.3f}")

"""しきい値チューニングで一気に Recall が復活

いまの数字の意味
Dev 最適閾値（ロジット） = −0.203 → 確率にすると ≈ 0.45（sigmoid(-0.203)）。
つまり「0.5より少し緩め」にすると陽性をよく拾える。

Dev: P=0.307 / R=0.893 / F1=0.457

Test: P=0.272 / R=0.883 / F1=0.416

Accuracy が下がったのは仕様：陽性をたくさん出す設定だから、負例が多いデータで Acc は落ちます。

AUC は 0.57–0.59：スコア自体はランダムより上。しきい値で実運用の好み（Recall優先）に寄せるのが妥当。

すぐやる実務設定
運用ターゲットに合わせて threshold を固定：

Recall重視なら今の logit = −0.203（p≈0.45） を採用

Precision もう少し欲しいなら、Dev の PR 曲線から「P≥0.4」等の点を選んで閾値を少し上げる
"""

# 使い回し用スニペット
BEST_LOGIT_TH = -0.203         # Devで得た値
BEST_PROBA_TH = 1/(1+np.exp(-BEST_LOGIT_TH))  # ≈ 0.45

preds = trainer.predict(test_ds)
scores = preds.predictions[:,1]              # positive のロジット
y_hat  = (scores >= BEST_LOGIT_TH).astype(int)
# or 確率で管理したいなら:
# y_hat = (scipy.special.expit(scores) >= BEST_PROBA_TH).astype(int)

"""次の改善（モデル側）
今は“文まるごと512”で、h/t 位置情報がありません。ここをいじるとPrecisionが上がりやすいです。

エンティティを明示マーキング
[E1] head [/E1] ... [E2] tail [/E2] を文中に挿入して再学習。

文をh/t近傍にクロップ
h/t の最短文や±N文だけに絞る（ノイズ減＆精度↑）。

不均衡対策の見直し
pos_weight をもう少し強める、あるいは負例を軽くダウンサンプル。

ハイパラ
A100なら batch=32, accum=1, epoch=3、LR 1e-5 なども試す価値あり。

結論：しきい値 0.45 相当で Recall を確保できました。
この設定で一旦パイプラインを回しつつ、上の 1)–2)（エンティティマーキング＋クロップ）を入れて再学習すれば、Precision を引き上げつつ F1 をさらに伸ばせるはずです。必要なら、その前処理コードもすぐ書きます。
"""

# しきい値チューニングや PR カーブを描く場合
from sklearn.metrics import precision_recall_curve
preds = trainer.predict(dev_ds)
scores = preds.predictions[:, 1]
labels = preds.label_ids
prec, rec, ths = precision_recall_curve(labels, scores)

# Dev のスコアから PR 曲線を作る下準備
# 最適なしきい値を求める→可視化→その閾値でDev/Testを再評価
# trainer と dev_ds/test_ds が用意済み前提

# === DevでPR曲線 → 最適しきい値決定 → 可視化 → Dev/Test再評価 一括セル ===
import numpy as np, matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, precision_recall_fscore_support, accuracy_score, roc_auc_score, average_precision_score

# 1) Devのスコア取得（※scoresはpositiveクラスのロジット想定）
preds = trainer.predict(dev_ds)
scores = preds.predictions[:, 1]
labels = preds.label_ids

# 2) PR曲線と平均適合率（AP）
prec, rec, ths = precision_recall_curve(labels, scores)
ap = average_precision_score(labels, scores)

# 3) F1最大のしきい値を選ぶ
f1s = 2 * prec * rec / (prec + rec + 1e-12)
best = np.nanargmax(f1s)
best_th = ths[best]                      # ← ロジットの閾値
best_p, best_r, best_f1 = prec[best], rec[best], f1s[best]
print(f"[Dev] Best logit threshold = {best_th:.3f} | P={best_p:.3f} R={best_r:.3f} F1={best_f1:.3f} | AP={ap:.3f}")

# 4) 可視化（ベスト点をマーキング）
plt.figure()
plt.plot(rec, prec, label=f"PR curve (AP={ap:.3f})")
plt.scatter([best_r], [best_p], s=60, marker='o', label=f"Best F1@{best_th:.3f}")
plt.xlabel("Recall"); plt.ylabel("Precision"); plt.title("Precision–Recall (Dev)")
plt.grid(True); plt.legend(); plt.show()

# 5) Devをベスト閾値で再評価
y_hat_dev = (scores >= best_th).astype(int)
p,r,f1,_ = precision_recall_fscore_support(labels, y_hat_dev, average="binary", zero_division=0)
acc = accuracy_score(labels, y_hat_dev); auc = roc_auc_score(labels, scores)
print(f"[Dev @ {best_th:.3f}] P={p:.3f} R={r:.3f} F1={f1:.3f} Acc={acc:.3f} AUC={auc:.3f}")

# 6) Testにも同じ閾値を適用（過学習回避の基本）
test_pred = trainer.predict(test_ds)
test_scores = test_pred.predictions[:, 1]
test_labels = test_pred.label_ids
y_hat_test = (test_scores >= best_th).astype(int)
p,r,f1,_ = precision_recall_fscore_support(test_labels, y_hat_test, average="binary", zero_division=0)
acc = accuracy_score(test_labels, y_hat_test); auc = roc_auc_score(test_labels, test_scores)
print(f"[Test @ {best_th:.3f}] P={p:.3f} R={r:.3f} F1={f1:.3f} Acc={acc:.3f} AUC={auc:.3f}")

# （必要なら）図をDriveに保存
# import os
# out_png = "/content/drive/MyDrive/BioRED/pr_curve_dev.png"
# plt.figure(); plt.plot(rec, prec); plt.scatter([best_r],[best_p],s=60); plt.xlabel("Recall"); plt.ylabel("Precision"); plt.grid(True); plt.savefig(out_png, dpi=150); print("Saved:", out_png)

"""グラフいい。読み取りとしては：

AP=0.319 → 不均衡データでは ROC より PR が重要。ランダム(≒陽性率)より上で、学習はできてますが、まだ余地あり。

曲線は P≈0.3 で長くフラット、右端（高 Recall）まで続いています＝閾値を下げれば陽性をよく拾えるタイプ。

ベストF1は logit = −0.203（確率≈0.45） 付近：Recall ≈ 0.89 / Precision ≈ 0.31。
目的が「見落としを減らす（Recall重視）」なら妥当な動作点です。
"""

# 1) 閾値を確定して、Dev/Test を再評価（表で確認）
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score

def eval_at_threshold(scores, labels, th):
    yhat = (scores >= th).astype(int)
    p,r,f1,_ = precision_recall_fscore_support(labels, yhat, average="binary", zero_division=0)
    acc = accuracy_score(labels, yhat)
    return p,r,f1,acc

# しきい値候補（確率ではなくロジット前提）
candidates = np.linspace(-1.0, 0.5, 13)  # お好みで変更
dev_pred = trainer.predict(dev_ds); dev_scores = dev_pred.predictions[:,1]; dev_labels = dev_pred.label_ids

rows = []
for th in candidates:
    p,r,f1,acc = eval_at_threshold(dev_scores, dev_labels, th)
    rows.append((th, p, r, f1, acc))
for th,p,r,f1,acc in rows:
    print(f"th={th:+.3f} | P={p:.3f} R={r:.3f} F1={f1:.3f} Acc={acc:.3f}")

"""いいデータ。表の読み方と結論。

何が分かる？
しきい値を下げるほど（−1.0 → 0.0）Recall は上がる／Accuracy は下がる。

F1 の山は −0.25 付近（0.449）。先ほど PR から求めた −0.203（F1=0.457） のほうが僅かに良いです。

つまり 「Dev で最適F1」を狙うなら −0.203 を採用、四捨五入して扱いやすくするなら −0.25 でもほぼ同等です。

目的が「因果文・トリプルを見落としたくない（Recall重視）」なら、
−0.25〜−0.20 のゾーンが妥当です（Recall ~0.9、Precision ~0.3）。

おすすめ運用
Dev で得た最適（−0.203）を採用 → Test にそのまま適用

もし「Precision をもう少し上げたい」要件が出たら、−0.125〜0.0 に上げる（F1はやや下るがAccは上がる）
"""

# 2) 選んだ閾値で Test を固定評価 & 保存
import pandas as pd
from scipy.special import expit

BEST_LOGIT_TH = -0.203  # ← Devで決めた値を入れる
# Testスコア
test_pred = trainer.predict(test_ds)
test_scores = test_pred.predictions[:,1]
test_labels = test_pred.label_ids
# メトリクス
p,r,f1,_ = precision_recall_fscore_support(test_labels, (test_scores>=BEST_LOGIT_TH).astype(int), average="binary", zero_division=0)
acc = accuracy_score(test_labels, (test_scores>=BEST_LOGIT_TH).astype(int))
auc = roc_auc_score(test_labels, test_scores)
print(f"[Test @{BEST_LOGIT_TH:+.3f}] P={p:.3f} R={r:.3f} F1={f1:.3f} Acc={acc:.3f} AUC={auc:.3f}")

# 予測の書き出し（文・h/t付きで見たい場合）
DATA_DIR = "/content/drive/MyDrive/BioRED"
raw = pd.read_csv(f"{DATA_DIR}/test_binary.tsv", sep="\t")  # 元の順序は保持されます
raw["logit"] = test_scores
raw["proba"] = expit(test_scores)
raw["pred"]  = (test_scores >= BEST_LOGIT_TH).astype(int)
raw.to_csv(f"{DATA_DIR}/test_pred_pubmedbert.csv", index=False)
print("Saved:", f"{DATA_DIR}/test_pred_pubmedbert.csv")

"""次の一手（精度をさらに上げたいなら）
エンティティの明示マーキング：[E1] h [/E1] ... [E2] t [/E2] を文に挿入して再学習（Precisionが上がりやすい）。

h/t 近傍にクロップ：対象文や前後±1文だけに絞る。

不均衡対策：pos_weight を少し強める or 負例を軽くダウンサンプル。

A100前提の再学習：batch=32, accum=1, epoch=3, lr=1e-5 を試す。

このまま「−0.203 固定」で回すか、「−0.25」に丸めるか、どちらにします？基準（Recall優先 or Precision少し上げる）を教えてくれたら最適点を決めます。
"""

# 3) サッとエラー分析（FP/FN を覗く）
fp = raw[(raw["label"]==0) & (raw["pred"]==1)].sample(20, random_state=0)
fn = raw[(raw["label"]==1) & (raw["pred"]==0)].sample(20, random_state=0)
fp[["sentence","h","t","proba"]].head(3), fn[["sentence","h","t","proba"]].head(3)

# FPが多いなら：Precision を上げたい → 閾値↑ or 前処理改善
# FNが多いなら：Recall を上げたい → 閾値↓ or モデル強化

"""出力はおそらく

左：FP（偽陽性）…pred=1 だけど label=0、proba ~0.50–0.58（境界付近）

右：FN（偽陰性）…pred=0 だけど label=1、proba ~0.41–0.43（やはり境界付近）

ここから読み取れること
どちらもしきい値付近の曖昧サンプルが多い → しきい値調整で裏返るタイプ。

FPの例に “patients” のような汎用名詞が混じる → ノイズ源になりやすい。

長文・相関語（association等）で因果と相関の混同が起きがち。



すぐ効く対策（学習し直し不要）
A) ルール系の後段フィルタ（Precision↑）
汎用エンティティの除外：{'patients','subjects','men','women','children'} 等を h/t に含む予測は弾く

因果キュー語のブースト：文に {'cause','induce','lead to','increase','decrease','due to'} などが無い場合は、しきい値を 0.05 引き上げる 等
"""

# 実行スニペット（確定しきい値で Test を評価＆保存）
import numpy as np, pandas as pd
from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score
from scipy.special import expit

BEST_LOGIT_TH = -0.203   # または -0.25 に差し替え

# Dev/Test スコア取得（すでに trainer がある前提）
dev_pred = trainer.predict(dev_ds)
test_pred = trainer.predict(test_ds)
dev_scores, dev_labels   = dev_pred.predictions[:,1], dev_pred.label_ids
test_scores, test_labels = test_pred.predictions[:,1], test_pred.label_ids

def eval_at(scores, labels, th):
    yhat = (scores >= th).astype(int)
    p,r,f1,_ = precision_recall_fscore_support(labels, yhat, average="binary", zero_division=0)
    acc = accuracy_score(labels, yhat)
    auc = roc_auc_score(labels, scores)
    return p,r,f1,acc,auc

print("[Dev]",  eval_at(dev_scores,  dev_labels,  BEST_LOGIT_TH))
print("[Test]", eval_at(test_scores, test_labels, BEST_LOGIT_TH))

# 予測を書き出し（分析用）
DATA_DIR = "/content/drive/MyDrive/BioRED"
raw = pd.read_csv(f"{DATA_DIR}/test_binary.tsv", sep="\t")
raw["logit"] = test_scores
raw["proba"] = expit(test_scores)
raw["pred"]  = (test_scores >= BEST_LOGIT_TH).astype(int)
raw.to_csv(f"{DATA_DIR}/test_pred_pubmedbert@{BEST_LOGIT_TH:+.3f}.csv", index=False)
print("Saved:", f"{DATA_DIR}/test_pred_pubmedbert@{BEST_LOGIT_TH:+.3f}.csv")

"""次の一手（精度をさらに上げたいなら）
エンティティの明示マーキング：[E1] h [/E1] ... [E2] t [/E2] を文に挿入して再学習（Precisionが上がりやすい）。

h/t 近傍にクロップ：対象文や前後±1文だけに絞る。

不均衡対策：pos_weight を少し強める or 負例を軽くダウンサンプル。

A100前提の再学習：batch=32, accum=1, epoch=3, lr=1e-5 を試す。

このまま「−0.203 固定」で回すか、「−0.25」に丸めるか、？基準（Recall優先 or Precision少し上げる）=>最適点を決めます。

狙いどおり Recall を大きく確保（~0.88）。Accuracy が下がっているのは、陽性を多めに出す閾値にした結果なので想定内。AUC は学習時と同程度で、しきい値調整で運用指標（F1/Recall）を改善できた形です。
"""

# CKPTを読み直した後に Drive へ退避
import os, json
SAVE_DIR = "/content/drive/MyDrive/BioRED/biored_pubmedbert_bin_ckpt15708"
os.makedirs(SAVE_DIR, exist_ok=True)

model.save_pretrained(SAVE_DIR)
tok.save_pretrained(SAVE_DIR)

# しきい値を保存（任意）
with open(os.path.join(SAVE_DIR, "threshold.json"), "w") as f:
    json.dump({"best_logit_th": float(best_th)}, f)

print("Saved to:", SAVE_DIR)

"""補足：学習再開を見越すなら trainer.save_state() も併せて保存しておくと、
trainer.train(resume_from_checkpoint=True) で 続きから再開できます。
"""

# PR曲線の描画は可視化・確認用のオプション。必須ではありません。
from sklearn.metrics import precision_recall_curve
import numpy as np

preds = trainer.predict(dev_ds)
scores = preds.predictions[:, 1]   # positive のスコア（ロジットでOK）
labels = preds.label_ids

prec, rec, ths = precision_recall_curve(labels, scores)
f1 = 2 * prec * rec / (prec + rec + 1e-12)
best = np.nanargmax(f1)
best_th = ths[best]
print(f"Best threshold (logit) = {best_th:.3f} | P={prec[best]:.3f} R={rec[best]:.3f} F1={f1[best]:.3f}")

# 決めたしきい値だけ保存する場合
import json, os
SAVE_DIR = "/content/drive/MyDrive/BioRED/models/pubmedbert_bin_best"
os.makedirs(SAVE_DIR, exist_ok=True)
with open(os.path.join(SAVE_DIR, "threshold.json"), "w") as f:
    json.dump({"best_logit_th": float(best_th)}, f)

# 上の続き、復帰時はこう読む
import json, os
with open(os.path.join(SAVE_DIR, "threshold.json")) as f:
    best_th = json.load(f)["best_logit_th"]

# 不要、PR曲線の描画セルで可視化用
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt

preds = trainer.predict(dev_ds)
scores = preds.predictions[:, 1]
labels = preds.label_ids

prec, rec, ths = precision_recall_curve(labels, scores)
plt.figure()
plt.plot(rec, prec)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision–Recall Curve on Dev Set')
plt.grid(True)
plt.show()